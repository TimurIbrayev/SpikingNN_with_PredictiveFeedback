
 Run on time: 2021-04-21 10:32:57.955827

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 dataset              : MNIST
	 batch_size           : 64
	 architecture         : VGG5_wFeedback
	 learning_rate        : 0.0001
	 pretrained_ann       : 
	 test_only            : False
	 epochs               : 100
	 lr_interval          : [60, 80, 90]
	 lr_reduce            : 10
	 optimizer            : Adam
	 weight_decay         : 0.0005
	 momentum             : 0.9
	 amsgrad              : True
	 dropout              : 0.2
	 kernel_size          : 3
	 devices              : 0
	 timesteps            : 1
	 trade_off            : 1.0
 DataParallel(
  (module): VGG_wFeedback(
    (features): Sequential(
      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.2, inplace=False)
      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): ReLU(inplace=True)
      (8): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (generate): Sequential(
      (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): LeakyReLU(negative_slope=0.01)
      (3): Dropout(p=0.2, inplace=False)
      (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): LeakyReLU(negative_slope=0.01)
      (6): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (7): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (8): Sigmoid()
    )
    (classifier): Sequential(
      (0): Linear(in_features=6272, out_features=4096, bias=False)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=4096, out_features=4096, bias=False)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=4096, out_features=10, bias=False)
    )
  )
)
 Adam (
Parameter Group 0
    amsgrad: True
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0005
)
 Epoch: 1, lr: 1.0e-04, train_loss: 0.3157 | 9.8468 | 10.1625, train_acc: 0.9011 test_loss: 0.1202 | 2.1394 | 2.2595, test_acc: 0.9612, best: 0.9612, time: 0:01:06
 Epoch: 2, lr: 1.0e-04, train_loss: 0.1385 | 2.2359 | 2.3744, train_acc: 0.9569 test_loss: 0.0944 | 1.6976 | 1.7920, test_acc: 0.9699, best: 0.9699, time: 0:01:05
 Epoch: 3, lr: 1.0e-04, train_loss: 0.1054 | 1.7923 | 1.8977, train_acc: 0.9678 test_loss: 0.0847 | 1.3852 | 1.4699, test_acc: 0.9740, best: 0.9740, time: 0:01:05
 Epoch: 4, lr: 1.0e-04, train_loss: 0.0893 | 1.5882 | 1.6774, train_acc: 0.9722 test_loss: 0.0792 | 1.2453 | 1.3244, test_acc: 0.9740, best: 0.9740, time: 0:00:57
 Epoch: 5, lr: 1.0e-04, train_loss: 0.0765 | 1.4620 | 1.5385, train_acc: 0.9766 test_loss: 0.0605 | 1.1323 | 1.1928, test_acc: 0.9797, best: 0.9797, time: 0:01:04
 Epoch: 6, lr: 1.0e-04, train_loss: 0.0686 | 1.3734 | 1.4420, train_acc: 0.9792 test_loss: 0.0660 | 1.0658 | 1.1318, test_acc: 0.9792, best: 0.9797, time: 0:00:57
 Epoch: 7, lr: 1.0e-04, train_loss: 0.0628 | 1.3031 | 1.3660, train_acc: 0.9812 test_loss: 0.0630 | 1.0069 | 1.0699, test_acc: 0.9780, best: 0.9797, time: 0:00:58
 Epoch: 8, lr: 1.0e-04, train_loss: 0.0589 | 1.2494 | 1.3083, train_acc: 0.9818 test_loss: 0.0572 | 0.9867 | 1.0438, test_acc: 0.9809, best: 0.9809, time: 0:01:06
 Epoch: 9, lr: 1.0e-04, train_loss: 0.0556 | 1.2038 | 1.2594, train_acc: 0.9830 test_loss: 0.0588 | 0.9696 | 1.0285, test_acc: 0.9824, best: 0.9824, time: 0:00:52
 Epoch: 10, lr: 1.0e-04, train_loss: 0.0525 | 1.1670 | 1.2195, train_acc: 0.9837 test_loss: 0.0486 | 0.9260 | 0.9746, test_acc: 0.9837, best: 0.9837, time: 0:00:45
 Epoch: 11, lr: 1.0e-04, train_loss: 0.0515 | 1.1337 | 1.1852, train_acc: 0.9845 test_loss: 0.0484 | 0.9137 | 0.9622, test_acc: 0.9843, best: 0.9843, time: 0:01:05
 Epoch: 12, lr: 1.0e-04, train_loss: 0.0481 | 1.1057 | 1.1538, train_acc: 0.9853 test_loss: 0.0428 | 0.8690 | 0.9118, test_acc: 0.9865, best: 0.9865, time: 0:01:05
 Epoch: 13, lr: 1.0e-04, train_loss: 0.0465 | 1.0803 | 1.1268, train_acc: 0.9861 test_loss: 0.0461 | 0.8697 | 0.9159, test_acc: 0.9849, best: 0.9865, time: 0:00:56
 Epoch: 14, lr: 1.0e-04, train_loss: 0.0443 | 1.0579 | 1.1022, train_acc: 0.9865 test_loss: 0.0497 | 0.8426 | 0.8923, test_acc: 0.9847, best: 0.9865, time: 0:00:57
 Epoch: 15, lr: 1.0e-04, train_loss: 0.0442 | 1.0376 | 1.0818, train_acc: 0.9869 test_loss: 0.0438 | 0.8133 | 0.8571, test_acc: 0.9852, best: 0.9865, time: 0:00:57
 Epoch: 16, lr: 1.0e-04, train_loss: 0.0421 | 1.0187 | 1.0608, train_acc: 0.9875 test_loss: 0.0440 | 0.8138 | 0.8578, test_acc: 0.9861, best: 0.9865, time: 0:00:58
 Epoch: 17, lr: 1.0e-04, train_loss: 0.0406 | 1.0023 | 1.0428, train_acc: 0.9880 test_loss: 0.0439 | 0.8047 | 0.8486, test_acc: 0.9857, best: 0.9865, time: 0:00:57
 Epoch: 18, lr: 1.0e-04, train_loss: 0.0407 | 0.9866 | 1.0273, train_acc: 0.9877 test_loss: 0.0426 | 0.7756 | 0.8182, test_acc: 0.9862, best: 0.9865, time: 0:00:57
 Epoch: 19, lr: 1.0e-04, train_loss: 0.0380 | 0.9716 | 1.0097, train_acc: 0.9883 test_loss: 0.0389 | 0.7724 | 0.8114, test_acc: 0.9856, best: 0.9865, time: 0:00:57
 Epoch: 20, lr: 1.0e-04, train_loss: 0.0382 | 0.9593 | 0.9975, train_acc: 0.9883 test_loss: 0.0415 | 0.7653 | 0.8068, test_acc: 0.9879, best: 0.9879, time: 0:01:05
 Epoch: 21, lr: 1.0e-04, train_loss: 0.0371 | 0.9461 | 0.9832, train_acc: 0.9887 test_loss: 0.0388 | 0.7454 | 0.7842, test_acc: 0.9877, best: 0.9879, time: 0:00:57
 Epoch: 22, lr: 1.0e-04, train_loss: 0.0377 | 0.9347 | 0.9723, train_acc: 0.9884 test_loss: 0.0366 | 0.7458 | 0.7824, test_acc: 0.9875, best: 0.9879, time: 0:00:57
 Epoch: 23, lr: 1.0e-04, train_loss: 0.0372 | 0.9241 | 0.9613, train_acc: 0.9885 test_loss: 0.0363 | 0.7543 | 0.7905, test_acc: 0.9882, best: 0.9882, time: 0:01:06
 Epoch: 24, lr: 1.0e-04, train_loss: 0.0344 | 0.9141 | 0.9484, train_acc: 0.9897 test_loss: 0.0352 | 0.7344 | 0.7697, test_acc: 0.9890, best: 0.9890, time: 0:01:05
 Epoch: 25, lr: 1.0e-04, train_loss: 0.0349 | 0.9053 | 0.9402, train_acc: 0.9889 test_loss: 0.0356 | 0.7251 | 0.7607, test_acc: 0.9885, best: 0.9890, time: 0:00:58
 Epoch: 26, lr: 1.0e-04, train_loss: 0.0328 | 0.8959 | 0.9287, train_acc: 0.9901 test_loss: 0.0345 | 0.7038 | 0.7384, test_acc: 0.9882, best: 0.9890, time: 0:00:58
 Epoch: 27, lr: 1.0e-04, train_loss: 0.0348 | 0.8878 | 0.9226, train_acc: 0.9890 test_loss: 0.0406 | 0.7106 | 0.7512, test_acc: 0.9868, best: 0.9890, time: 0:00:58
 Epoch: 28, lr: 1.0e-04, train_loss: 0.0339 | 0.8801 | 0.9141, train_acc: 0.9891 test_loss: 0.0405 | 0.7084 | 0.7489, test_acc: 0.9870, best: 0.9890, time: 0:00:58
 Epoch: 29, lr: 1.0e-04, train_loss: 0.0331 | 0.8725 | 0.9056, train_acc: 0.9898 test_loss: 0.0379 | 0.7033 | 0.7412, test_acc: 0.9877, best: 0.9890, time: 0:00:58
 Epoch: 30, lr: 1.0e-04, train_loss: 0.0327 | 0.8651 | 0.8979, train_acc: 0.9900 test_loss: 0.0377 | 0.6913 | 0.7290, test_acc: 0.9874, best: 0.9890, time: 0:00:58
 Epoch: 31, lr: 1.0e-04, train_loss: 0.0331 | 0.8583 | 0.8914, train_acc: 0.9900 test_loss: 0.0368 | 0.6883 | 0.7251, test_acc: 0.9873, best: 0.9890, time: 0:00:58
 Epoch: 32, lr: 1.0e-04, train_loss: 0.0317 | 0.8520 | 0.8837, train_acc: 0.9899 test_loss: 0.0336 | 0.6890 | 0.7226, test_acc: 0.9885, best: 0.9890, time: 0:00:58
 Epoch: 33, lr: 1.0e-04, train_loss: 0.0307 | 0.8456 | 0.8763, train_acc: 0.9905 test_loss: 0.0412 | 0.6960 | 0.7373, test_acc: 0.9879, best: 0.9890, time: 0:00:58
 Epoch: 34, lr: 1.0e-04, train_loss: 0.0305 | 0.8396 | 0.8702, train_acc: 0.9909 test_loss: 0.0385 | 0.6871 | 0.7256, test_acc: 0.9873, best: 0.9890, time: 0:00:58
 Epoch: 35, lr: 1.0e-04, train_loss: 0.0319 | 0.8338 | 0.8657, train_acc: 0.9896 test_loss: 0.0380 | 0.6732 | 0.7111, test_acc: 0.9872, best: 0.9890, time: 0:00:57
 Epoch: 36, lr: 1.0e-04, train_loss: 0.0303 | 0.8284 | 0.8588, train_acc: 0.9905 test_loss: 0.0338 | 0.6691 | 0.7029, test_acc: 0.9892, best: 0.9892, time: 0:01:05
 Epoch: 37, lr: 1.0e-04, train_loss: 0.0289 | 0.8229 | 0.8518, train_acc: 0.9913 test_loss: 0.0331 | 0.6609 | 0.6940, test_acc: 0.9888, best: 0.9892, time: 0:00:58
 Epoch: 38, lr: 1.0e-04, train_loss: 0.0302 | 0.8175 | 0.8477, train_acc: 0.9909 test_loss: 0.0337 | 0.6642 | 0.6979, test_acc: 0.9893, best: 0.9893, time: 0:01:01
 Epoch: 39, lr: 1.0e-04, train_loss: 0.0302 | 0.8135 | 0.8437, train_acc: 0.9907 test_loss: 0.0341 | 0.6527 | 0.6868, test_acc: 0.9879, best: 0.9893, time: 0:00:41
 Epoch: 40, lr: 1.0e-04, train_loss: 0.0300 | 0.8082 | 0.8381, train_acc: 0.9902 test_loss: 0.0348 | 0.6535 | 0.6883, test_acc: 0.9880, best: 0.9893, time: 0:00:44
 Epoch: 41, lr: 1.0e-04, train_loss: 0.0308 | 0.8039 | 0.8347, train_acc: 0.9902 test_loss: 0.0369 | 0.6587 | 0.6955, test_acc: 0.9894, best: 0.9894, time: 0:01:05
 Epoch: 42, lr: 1.0e-04, train_loss: 0.0290 | 0.7991 | 0.8281, train_acc: 0.9911 test_loss: 0.0331 | 0.6516 | 0.6847, test_acc: 0.9891, best: 0.9894, time: 0:00:57
 Epoch: 43, lr: 1.0e-04, train_loss: 0.0280 | 0.7951 | 0.8231, train_acc: 0.9913 test_loss: 0.0323 | 0.6411 | 0.6734, test_acc: 0.9900, best: 0.9900, time: 0:01:05
 Epoch: 44, lr: 1.0e-04, train_loss: 0.0283 | 0.7906 | 0.8189, train_acc: 0.9909 test_loss: 0.0323 | 0.6511 | 0.6834, test_acc: 0.9882, best: 0.9900, time: 0:00:57
 Epoch: 45, lr: 1.0e-04, train_loss: 0.0276 | 0.7864 | 0.8139, train_acc: 0.9910 test_loss: 0.0320 | 0.6394 | 0.6714, test_acc: 0.9896, best: 0.9900, time: 0:00:57
 Epoch: 46, lr: 1.0e-04, train_loss: 0.0281 | 0.7828 | 0.8109, train_acc: 0.9909 test_loss: 0.0368 | 0.6431 | 0.6799, test_acc: 0.9877, best: 0.9900, time: 0:00:58
 Epoch: 47, lr: 1.0e-04, train_loss: 0.0283 | 0.7791 | 0.8074, train_acc: 0.9909 test_loss: 0.0322 | 0.6481 | 0.6803, test_acc: 0.9898, best: 0.9900, time: 0:00:57
 Epoch: 48, lr: 1.0e-04, train_loss: 0.0289 | 0.7755 | 0.8044, train_acc: 0.9907 test_loss: 0.0346 | 0.6344 | 0.6689, test_acc: 0.9883, best: 0.9900, time: 0:00:58
 Epoch: 49, lr: 1.0e-04, train_loss: 0.0293 | 0.7722 | 0.8014, train_acc: 0.9906 test_loss: 0.0344 | 0.6440 | 0.6785, test_acc: 0.9886, best: 0.9900, time: 0:00:56
 Epoch: 50, lr: 1.0e-04, train_loss: 0.0273 | 0.7675 | 0.7948, train_acc: 0.9914 test_loss: 0.0356 | 0.6417 | 0.6772, test_acc: 0.9891, best: 0.9900, time: 0:00:56
 Epoch: 51, lr: 1.0e-04, train_loss: 0.0272 | 0.7648 | 0.7919, train_acc: 0.9912 test_loss: 0.0386 | 0.6271 | 0.6656, test_acc: 0.9877, best: 0.9900, time: 0:00:57
 Epoch: 52, lr: 1.0e-04, train_loss: 0.0272 | 0.7614 | 0.7886, train_acc: 0.9914 test_loss: 0.0305 | 0.6335 | 0.6640, test_acc: 0.9901, best: 0.9901, time: 0:01:05
 Epoch: 53, lr: 1.0e-04, train_loss: 0.0271 | 0.7584 | 0.7854, train_acc: 0.9914 test_loss: 0.0351 | 0.6255 | 0.6606, test_acc: 0.9881, best: 0.9901, time: 0:00:58
 Epoch: 54, lr: 1.0e-04, train_loss: 0.0269 | 0.7557 | 0.7826, train_acc: 0.9917 test_loss: 0.0333 | 0.6304 | 0.6637, test_acc: 0.9875, best: 0.9901, time: 0:00:58
 Epoch: 55, lr: 1.0e-04, train_loss: 0.0260 | 0.7524 | 0.7784, train_acc: 0.9916 test_loss: 0.0305 | 0.6187 | 0.6492, test_acc: 0.9898, best: 0.9901, time: 0:00:57
 Epoch: 56, lr: 1.0e-04, train_loss: 0.0259 | 0.7495 | 0.7754, train_acc: 0.9918 test_loss: 0.0345 | 0.6192 | 0.6537, test_acc: 0.9873, best: 0.9901, time: 0:00:58
 Epoch: 57, lr: 1.0e-04, train_loss: 0.0261 | 0.7470 | 0.7732, train_acc: 0.9917 test_loss: 0.0309 | 0.6172 | 0.6480, test_acc: 0.9893, best: 0.9901, time: 0:00:57
 Epoch: 58, lr: 1.0e-04, train_loss: 0.0259 | 0.7445 | 0.7705, train_acc: 0.9922 test_loss: 0.0330 | 0.6135 | 0.6465, test_acc: 0.9887, best: 0.9901, time: 0:00:57
 Epoch: 59, lr: 1.0e-04, train_loss: 0.0269 | 0.7415 | 0.7684, train_acc: 0.9913 test_loss: 0.0318 | 0.6151 | 0.6469, test_acc: 0.9897, best: 0.9901, time: 0:00:57
 Epoch: 60, lr: 1.0e-05, train_loss: 0.0161 | 0.7388 | 0.7549, train_acc: 0.9957 test_loss: 0.0263 | 0.6125 | 0.6388, test_acc: 0.9912, best: 0.9912, time: 0:01:05
 Epoch: 61, lr: 1.0e-05, train_loss: 0.0137 | 0.7384 | 0.7521, train_acc: 0.9964 test_loss: 0.0255 | 0.6122 | 0.6377, test_acc: 0.9911, best: 0.9912, time: 0:00:57
 Epoch: 62, lr: 1.0e-05, train_loss: 0.0126 | 0.7375 | 0.7501, train_acc: 0.9968 test_loss: 0.0256 | 0.6139 | 0.6395, test_acc: 0.9909, best: 0.9912, time: 0:00:57
 Epoch: 63, lr: 1.0e-05, train_loss: 0.0122 | 0.7374 | 0.7495, train_acc: 0.9970 test_loss: 0.0255 | 0.6102 | 0.6357, test_acc: 0.9912, best: 0.9912, time: 0:00:58
 Epoch: 64, lr: 1.0e-05, train_loss: 0.0117 | 0.7368 | 0.7486, train_acc: 0.9972 test_loss: 0.0256 | 0.6122 | 0.6377, test_acc: 0.9909, best: 0.9912, time: 0:00:57
 Epoch: 65, lr: 1.0e-05, train_loss: 0.0112 | 0.7366 | 0.7479, train_acc: 0.9976 test_loss: 0.0256 | 0.6151 | 0.6407, test_acc: 0.9914, best: 0.9914, time: 0:01:06
 Epoch: 66, lr: 1.0e-05, train_loss: 0.0110 | 0.7364 | 0.7474, train_acc: 0.9974 test_loss: 0.0251 | 0.6114 | 0.6364, test_acc: 0.9916, best: 0.9916, time: 0:01:05
 Epoch: 67, lr: 1.0e-05, train_loss: 0.0109 | 0.7360 | 0.7469, train_acc: 0.9975 test_loss: 0.0252 | 0.6111 | 0.6363, test_acc: 0.9912, best: 0.9916, time: 0:00:58
 Epoch: 68, lr: 1.0e-05, train_loss: 0.0106 | 0.7362 | 0.7468, train_acc: 0.9978 test_loss: 0.0254 | 0.6121 | 0.6375, test_acc: 0.9911, best: 0.9916, time: 0:00:54
 Epoch: 69, lr: 1.0e-05, train_loss: 0.0105 | 0.7357 | 0.7462, train_acc: 0.9980 test_loss: 0.0253 | 0.6118 | 0.6371, test_acc: 0.9912, best: 0.9916, time: 0:00:41
 Epoch: 70, lr: 1.0e-05, train_loss: 0.0104 | 0.7355 | 0.7460, train_acc: 0.9980 test_loss: 0.0253 | 0.6120 | 0.6373, test_acc: 0.9917, best: 0.9917, time: 0:00:49
 Epoch: 71, lr: 1.0e-05, train_loss: 0.0106 | 0.7348 | 0.7454, train_acc: 0.9977 test_loss: 0.0252 | 0.6098 | 0.6350, test_acc: 0.9917, best: 0.9917, time: 0:00:57
 Epoch: 72, lr: 1.0e-05, train_loss: 0.0106 | 0.7348 | 0.7454, train_acc: 0.9979 test_loss: 0.0254 | 0.6108 | 0.6362, test_acc: 0.9915, best: 0.9917, time: 0:00:58
 Epoch: 73, lr: 1.0e-05, train_loss: 0.0107 | 0.7344 | 0.7452, train_acc: 0.9979 test_loss: 0.0253 | 0.6103 | 0.6356, test_acc: 0.9917, best: 0.9917, time: 0:00:57
 Epoch: 74, lr: 1.0e-05, train_loss: 0.0106 | 0.7341 | 0.7447, train_acc: 0.9979 test_loss: 0.0250 | 0.6089 | 0.6338, test_acc: 0.9918, best: 0.9918, time: 0:01:06
 Epoch: 75, lr: 1.0e-05, train_loss: 0.0106 | 0.7337 | 0.7443, train_acc: 0.9979 test_loss: 0.0253 | 0.6096 | 0.6350, test_acc: 0.9917, best: 0.9918, time: 0:00:57
 Epoch: 76, lr: 1.0e-05, train_loss: 0.0107 | 0.7338 | 0.7445, train_acc: 0.9981 test_loss: 0.0254 | 0.6096 | 0.6350, test_acc: 0.9913, best: 0.9918, time: 0:00:58
 Epoch: 77, lr: 1.0e-05, train_loss: 0.0108 | 0.7335 | 0.7443, train_acc: 0.9979 test_loss: 0.0254 | 0.6117 | 0.6371, test_acc: 0.9916, best: 0.9918, time: 0:00:57
 Epoch: 78, lr: 1.0e-05, train_loss: 0.0108 | 0.7330 | 0.7438, train_acc: 0.9981 test_loss: 0.0252 | 0.6111 | 0.6364, test_acc: 0.9918, best: 0.9918, time: 0:00:57
 Epoch: 79, lr: 1.0e-05, train_loss: 0.0108 | 0.7330 | 0.7438, train_acc: 0.9979 test_loss: 0.0254 | 0.6098 | 0.6352, test_acc: 0.9915, best: 0.9918, time: 0:00:58
 Epoch: 80, lr: 1.0e-06, train_loss: 0.0100 | 0.7326 | 0.7427, train_acc: 0.9984 test_loss: 0.0253 | 0.6094 | 0.6347, test_acc: 0.9917, best: 0.9918, time: 0:00:57
 Epoch: 81, lr: 1.0e-06, train_loss: 0.0103 | 0.7325 | 0.7428, train_acc: 0.9981 test_loss: 0.0254 | 0.6092 | 0.6346, test_acc: 0.9917, best: 0.9918, time: 0:00:58
 Epoch: 82, lr: 1.0e-06, train_loss: 0.0101 | 0.7326 | 0.7426, train_acc: 0.9981 test_loss: 0.0253 | 0.6091 | 0.6343, test_acc: 0.9917, best: 0.9918, time: 0:00:57
 Epoch: 83, lr: 1.0e-06, train_loss: 0.0106 | 0.7325 | 0.7431, train_acc: 0.9980 test_loss: 0.0253 | 0.6083 | 0.6336, test_acc: 0.9915, best: 0.9918, time: 0:00:58
 Epoch: 84, lr: 1.0e-06, train_loss: 0.0102 | 0.7326 | 0.7429, train_acc: 0.9983 test_loss: 0.0253 | 0.6085 | 0.6339, test_acc: 0.9916, best: 0.9918, time: 0:00:58
 Epoch: 85, lr: 1.0e-06, train_loss: 0.0103 | 0.7324 | 0.7427, train_acc: 0.9985 test_loss: 0.0253 | 0.6081 | 0.6334, test_acc: 0.9917, best: 0.9918, time: 0:00:57
 Epoch: 86, lr: 1.0e-06, train_loss: 0.0099 | 0.7325 | 0.7425, train_acc: 0.9984 test_loss: 0.0253 | 0.6090 | 0.6343, test_acc: 0.9915, best: 0.9918, time: 0:00:58
 Epoch: 87, lr: 1.0e-06, train_loss: 0.0099 | 0.7323 | 0.7423, train_acc: 0.9983 test_loss: 0.0253 | 0.6091 | 0.6344, test_acc: 0.9916, best: 0.9918, time: 0:00:57
 Epoch: 88, lr: 1.0e-06, train_loss: 0.0099 | 0.7326 | 0.7425, train_acc: 0.9983 test_loss: 0.0252 | 0.6092 | 0.6344, test_acc: 0.9918, best: 0.9918, time: 0:00:58
 Epoch: 89, lr: 1.0e-06, train_loss: 0.0099 | 0.7323 | 0.7422, train_acc: 0.9984 test_loss: 0.0252 | 0.6091 | 0.6343, test_acc: 0.9918, best: 0.9918, time: 0:00:57
 Epoch: 90, lr: 1.0e-07, train_loss: 0.0097 | 0.7323 | 0.7421, train_acc: 0.9985 test_loss: 0.0252 | 0.6091 | 0.6343, test_acc: 0.9917, best: 0.9918, time: 0:00:56
 Epoch: 91, lr: 1.0e-07, train_loss: 0.0098 | 0.7326 | 0.7424, train_acc: 0.9984 test_loss: 0.0252 | 0.6087 | 0.6340, test_acc: 0.9919, best: 0.9919, time: 0:01:05
 Epoch: 92, lr: 1.0e-07, train_loss: 0.0100 | 0.7324 | 0.7424, train_acc: 0.9985 test_loss: 0.0252 | 0.6088 | 0.6341, test_acc: 0.9919, best: 0.9919, time: 0:00:58
 Epoch: 93, lr: 1.0e-07, train_loss: 0.0104 | 0.7324 | 0.7428, train_acc: 0.9981 test_loss: 0.0252 | 0.6089 | 0.6341, test_acc: 0.9919, best: 0.9919, time: 0:00:57
 Epoch: 94, lr: 1.0e-07, train_loss: 0.0097 | 0.7323 | 0.7420, train_acc: 0.9986 test_loss: 0.0252 | 0.6089 | 0.6341, test_acc: 0.9919, best: 0.9919, time: 0:00:57
 Epoch: 95, lr: 1.0e-07, train_loss: 0.0098 | 0.7325 | 0.7423, train_acc: 0.9984 test_loss: 0.0252 | 0.6088 | 0.6340, test_acc: 0.9919, best: 0.9919, time: 0:00:57
 Epoch: 96, lr: 1.0e-07, train_loss: 0.0100 | 0.7323 | 0.7423, train_acc: 0.9983 test_loss: 0.0252 | 0.6089 | 0.6341, test_acc: 0.9918, best: 0.9919, time: 0:00:57
 Epoch: 97, lr: 1.0e-07, train_loss: 0.0101 | 0.7320 | 0.7421, train_acc: 0.9983 test_loss: 0.0252 | 0.6091 | 0.6343, test_acc: 0.9918, best: 0.9919, time: 0:00:58
 Epoch: 98, lr: 1.0e-07, train_loss: 0.0100 | 0.7324 | 0.7424, train_acc: 0.9984 test_loss: 0.0252 | 0.6089 | 0.6341, test_acc: 0.9918, best: 0.9919, time: 0:00:57
 Epoch: 99, lr: 1.0e-07, train_loss: 0.0101 | 0.7324 | 0.7425, train_acc: 0.9982 test_loss: 0.0252 | 0.6088 | 0.6340, test_acc: 0.9918, best: 0.9919, time: 0:00:46
 Highest accuracy: 0.9919