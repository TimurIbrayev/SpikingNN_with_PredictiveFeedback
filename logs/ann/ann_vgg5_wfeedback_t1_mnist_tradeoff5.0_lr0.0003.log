
 Run on time: 2021-04-22 07:01:59.712007

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 dataset              : MNIST
	 batch_size           : 64
	 architecture         : VGG5_wFeedback
	 learning_rate        : 0.0003
	 pretrained_ann       : 
	 test_only            : False
	 epochs               : 100
	 lr_interval          : [60, 80, 90]
	 lr_reduce            : 10
	 optimizer            : Adam
	 weight_decay         : 0.0005
	 momentum             : 0.9
	 amsgrad              : True
	 dropout              : 0.2
	 kernel_size          : 3
	 devices              : 0
	 timesteps            : 1
	 trade_off            : 5.0
 DataParallel(
  (module): VGG_wFeedback(
    (features): Sequential(
      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.2, inplace=False)
      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): ReLU(inplace=True)
      (8): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)
    )
    (generate): Sequential(
      (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (2): LeakyReLU(negative_slope=0.01)
      (3): Dropout(p=0.2, inplace=False)
      (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (5): LeakyReLU(negative_slope=0.01)
      (6): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
      (7): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (8): Sigmoid()
    )
    (classifier): Sequential(
      (0): Linear(in_features=6272, out_features=4096, bias=False)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=4096, out_features=4096, bias=False)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=4096, out_features=10, bias=False)
    )
  )
)
 Adam (
Parameter Group 0
    amsgrad: True
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0003
    weight_decay: 0.0005
)
 Epoch: 1, lr: 3.0e-04, train_loss: 0.2801 | 5.3900 | 27.2301, train_acc: 0.9103 test_loss: 0.1186 | 1.4056 | 7.1467, test_acc: 0.9630, best: 0.9630, time: 0:00:36
 Epoch: 2, lr: 3.0e-04, train_loss: 0.1562 | 1.5091 | 7.7017, train_acc: 0.9508 test_loss: 0.1055 | 1.1141 | 5.6759, test_acc: 0.9685, best: 0.9685, time: 0:00:36
 Epoch: 3, lr: 3.0e-04, train_loss: 0.1313 | 1.2536 | 6.3991, train_acc: 0.9589 test_loss: 0.1281 | 0.9619 | 4.9375, test_acc: 0.9598, best: 0.9685, time: 0:00:29
 Epoch: 4, lr: 3.0e-04, train_loss: 0.1157 | 1.1195 | 5.7133, train_acc: 0.9635 test_loss: 0.0994 | 0.8527 | 4.3628, test_acc: 0.9689, best: 0.9689, time: 0:00:36
 Epoch: 5, lr: 3.0e-04, train_loss: 0.1097 | 1.0328 | 5.2738, train_acc: 0.9648 test_loss: 0.0868 | 0.7884 | 4.0287, test_acc: 0.9728, best: 0.9728, time: 0:00:36
 Epoch: 6, lr: 3.0e-04, train_loss: 0.1030 | 0.9739 | 4.9723, train_acc: 0.9673 test_loss: 0.0804 | 0.7588 | 3.8743, test_acc: 0.9744, best: 0.9744, time: 0:00:37
 Epoch: 7, lr: 3.0e-04, train_loss: 0.0993 | 0.9285 | 4.7416, train_acc: 0.9687 test_loss: 0.0793 | 0.7235 | 3.6968, test_acc: 0.9744, best: 0.9744, time: 0:00:29
 Epoch: 8, lr: 3.0e-04, train_loss: 0.0943 | 0.8945 | 4.5667, train_acc: 0.9707 test_loss: 0.0736 | 0.7075 | 3.6110, test_acc: 0.9774, best: 0.9774, time: 0:00:36
 Epoch: 9, lr: 3.0e-04, train_loss: 0.0939 | 0.8669 | 4.4286, train_acc: 0.9704 test_loss: 0.0799 | 0.6960 | 3.5599, test_acc: 0.9744, best: 0.9774, time: 0:00:29
 Epoch: 10, lr: 3.0e-04, train_loss: 0.0898 | 0.8445 | 4.3125, train_acc: 0.9714 test_loss: 0.0725 | 0.6798 | 3.4717, test_acc: 0.9765, best: 0.9774, time: 0:00:29
 Epoch: 11, lr: 3.0e-04, train_loss: 0.0876 | 0.8258 | 4.2168, train_acc: 0.9726 test_loss: 0.0671 | 0.6699 | 3.4166, test_acc: 0.9780, best: 0.9780, time: 0:00:36
 Epoch: 12, lr: 3.0e-04, train_loss: 0.0878 | 0.8102 | 4.1387, train_acc: 0.9728 test_loss: 0.0637 | 0.6634 | 3.3807, test_acc: 0.9804, best: 0.9804, time: 0:00:36
 Epoch: 13, lr: 3.0e-04, train_loss: 0.0865 | 0.7960 | 4.0663, train_acc: 0.9730 test_loss: 0.0695 | 0.6553 | 3.3460, test_acc: 0.9763, best: 0.9804, time: 0:00:29
 Epoch: 14, lr: 3.0e-04, train_loss: 0.0815 | 0.7831 | 3.9968, train_acc: 0.9737 test_loss: 0.0841 | 0.6419 | 3.2934, test_acc: 0.9749, best: 0.9804, time: 0:00:29
 Epoch: 15, lr: 3.0e-04, train_loss: 0.0809 | 0.7710 | 3.9357, train_acc: 0.9743 test_loss: 0.0703 | 0.6441 | 3.2908, test_acc: 0.9768, best: 0.9804, time: 0:00:29
 Epoch: 16, lr: 3.0e-04, train_loss: 0.0806 | 0.7604 | 3.8826, train_acc: 0.9746 test_loss: 0.0640 | 0.6317 | 3.2224, test_acc: 0.9794, best: 0.9804, time: 0:00:29
 Epoch: 17, lr: 3.0e-04, train_loss: 0.0768 | 0.7508 | 3.8307, train_acc: 0.9760 test_loss: 0.0627 | 0.6153 | 3.1390, test_acc: 0.9804, best: 0.9804, time: 0:00:29
 Epoch: 18, lr: 3.0e-04, train_loss: 0.0744 | 0.7421 | 3.7847, train_acc: 0.9758 test_loss: 0.0637 | 0.6159 | 3.1429, test_acc: 0.9793, best: 0.9804, time: 0:00:29
 Epoch: 19, lr: 3.0e-04, train_loss: 0.0759 | 0.7342 | 3.7468, train_acc: 0.9763 test_loss: 0.0656 | 0.6118 | 3.1245, test_acc: 0.9793, best: 0.9804, time: 0:00:29
 Epoch: 20, lr: 3.0e-04, train_loss: 0.0725 | 0.7263 | 3.7042, train_acc: 0.9771 test_loss: 0.0607 | 0.6128 | 3.1246, test_acc: 0.9809, best: 0.9809, time: 0:00:36
 Epoch: 21, lr: 3.0e-04, train_loss: 0.0728 | 0.7197 | 3.6712, train_acc: 0.9771 test_loss: 0.0573 | 0.6106 | 3.1105, test_acc: 0.9817, best: 0.9817, time: 0:00:37
 Epoch: 22, lr: 3.0e-04, train_loss: 0.0716 | 0.7133 | 3.6381, train_acc: 0.9780 test_loss: 0.0556 | 0.5950 | 3.0306, test_acc: 0.9823, best: 0.9823, time: 0:00:36
 Epoch: 23, lr: 3.0e-04, train_loss: 0.0665 | 0.7072 | 3.6027, train_acc: 0.9787 test_loss: 0.0562 | 0.5930 | 3.0214, test_acc: 0.9825, best: 0.9825, time: 0:00:36
 Epoch: 24, lr: 3.0e-04, train_loss: 0.0670 | 0.7016 | 3.5751, train_acc: 0.9789 test_loss: 0.0575 | 0.5950 | 3.0324, test_acc: 0.9814, best: 0.9825, time: 0:00:29
 Epoch: 25, lr: 3.0e-04, train_loss: 0.0691 | 0.6972 | 3.5549, train_acc: 0.9780 test_loss: 0.0564 | 0.5849 | 2.9809, test_acc: 0.9825, best: 0.9825, time: 0:00:29
 Epoch: 26, lr: 3.0e-04, train_loss: 0.0656 | 0.6916 | 3.5238, train_acc: 0.9798 test_loss: 0.0505 | 0.5775 | 2.9380, test_acc: 0.9842, best: 0.9842, time: 0:00:36
 Epoch: 27, lr: 3.0e-04, train_loss: 0.0630 | 0.6873 | 3.4994, train_acc: 0.9802 test_loss: 0.0564 | 0.5775 | 2.9438, test_acc: 0.9812, best: 0.9842, time: 0:00:29
 Epoch: 28, lr: 3.0e-04, train_loss: 0.0639 | 0.6824 | 3.4757, train_acc: 0.9797 test_loss: 0.0578 | 0.5747 | 2.9312, test_acc: 0.9812, best: 0.9842, time: 0:00:29
 Epoch: 29, lr: 3.0e-04, train_loss: 0.0633 | 0.6783 | 3.4546, train_acc: 0.9796 test_loss: 0.0519 | 0.5769 | 2.9365, test_acc: 0.9832, best: 0.9842, time: 0:00:29
 Epoch: 30, lr: 3.0e-04, train_loss: 0.0591 | 0.6749 | 3.4335, train_acc: 0.9814 test_loss: 0.0566 | 0.5792 | 2.9527, test_acc: 0.9817, best: 0.9842, time: 0:00:29
 Epoch: 31, lr: 3.0e-04, train_loss: 0.0608 | 0.6706 | 3.4138, train_acc: 0.9804 test_loss: 0.0553 | 0.5669 | 2.8897, test_acc: 0.9830, best: 0.9842, time: 0:00:29
 Epoch: 32, lr: 3.0e-04, train_loss: 0.0582 | 0.6672 | 3.3943, train_acc: 0.9814 test_loss: 0.0528 | 0.5698 | 2.9017, test_acc: 0.9831, best: 0.9842, time: 0:00:29
 Epoch: 33, lr: 3.0e-04, train_loss: 0.0577 | 0.6639 | 3.3771, train_acc: 0.9813 test_loss: 0.0590 | 0.5663 | 2.8905, test_acc: 0.9815, best: 0.9842, time: 0:00:29
 Epoch: 34, lr: 3.0e-04, train_loss: 0.0571 | 0.6608 | 3.3608, train_acc: 0.9823 test_loss: 0.0559 | 0.5683 | 2.8976, test_acc: 0.9816, best: 0.9842, time: 0:00:29
 Epoch: 35, lr: 3.0e-04, train_loss: 0.0581 | 0.6572 | 3.3439, train_acc: 0.9821 test_loss: 0.0456 | 0.5704 | 2.8975, test_acc: 0.9862, best: 0.9862, time: 0:00:36
 Epoch: 36, lr: 3.0e-04, train_loss: 0.0579 | 0.6547 | 3.3313, train_acc: 0.9813 test_loss: 0.0484 | 0.5632 | 2.8644, test_acc: 0.9852, best: 0.9862, time: 0:00:29
 Epoch: 37, lr: 3.0e-04, train_loss: 0.0538 | 0.6517 | 3.3123, train_acc: 0.9828 test_loss: 0.0440 | 0.5685 | 2.8868, test_acc: 0.9857, best: 0.9862, time: 0:00:29
 Epoch: 38, lr: 3.0e-04, train_loss: 0.0547 | 0.6493 | 3.3010, train_acc: 0.9826 test_loss: 0.0471 | 0.5589 | 2.8415, test_acc: 0.9845, best: 0.9862, time: 0:00:30
 Epoch: 39, lr: 3.0e-04, train_loss: 0.0544 | 0.6466 | 3.2873, train_acc: 0.9831 test_loss: 0.0441 | 0.5573 | 2.8305, test_acc: 0.9863, best: 0.9863, time: 0:00:37
 Epoch: 40, lr: 3.0e-04, train_loss: 0.0528 | 0.6440 | 3.2726, train_acc: 0.9831 test_loss: 0.0440 | 0.5652 | 2.8702, test_acc: 0.9858, best: 0.9863, time: 0:00:29
 Epoch: 41, lr: 3.0e-04, train_loss: 0.0533 | 0.6415 | 3.2610, train_acc: 0.9832 test_loss: 0.0480 | 0.5579 | 2.8375, test_acc: 0.9852, best: 0.9863, time: 0:00:29
 Epoch: 42, lr: 3.0e-04, train_loss: 0.0539 | 0.6394 | 3.2511, train_acc: 0.9827 test_loss: 0.0513 | 0.5541 | 2.8216, test_acc: 0.9836, best: 0.9863, time: 0:00:29
 Epoch: 43, lr: 3.0e-04, train_loss: 0.0522 | 0.6372 | 3.2380, train_acc: 0.9831 test_loss: 0.0451 | 0.5501 | 2.7958, test_acc: 0.9857, best: 0.9863, time: 0:00:29
 Epoch: 44, lr: 3.0e-04, train_loss: 0.0524 | 0.6352 | 3.2283, train_acc: 0.9833 test_loss: 0.0406 | 0.5521 | 2.8012, test_acc: 0.9869, best: 0.9869, time: 0:00:36
 Epoch: 45, lr: 3.0e-04, train_loss: 0.0545 | 0.6325 | 3.2172, train_acc: 0.9822 test_loss: 0.0432 | 0.5505 | 2.7957, test_acc: 0.9848, best: 0.9869, time: 0:00:29
 Epoch: 46, lr: 3.0e-04, train_loss: 0.0513 | 0.6308 | 3.2050, train_acc: 0.9839 test_loss: 0.0425 | 0.5493 | 2.7890, test_acc: 0.9855, best: 0.9869, time: 0:00:29
 Epoch: 47, lr: 3.0e-04, train_loss: 0.0512 | 0.6292 | 3.1972, train_acc: 0.9832 test_loss: 0.0512 | 0.5443 | 2.7726, test_acc: 0.9838, best: 0.9869, time: 0:00:29
 Epoch: 48, lr: 3.0e-04, train_loss: 0.0516 | 0.6274 | 3.1888, train_acc: 0.9834 test_loss: 0.0416 | 0.5481 | 2.7821, test_acc: 0.9855, best: 0.9869, time: 0:00:29
 Epoch: 49, lr: 3.0e-04, train_loss: 0.0491 | 0.6255 | 3.1765, train_acc: 0.9837 test_loss: 0.0501 | 0.5545 | 2.8223, test_acc: 0.9837, best: 0.9869, time: 0:00:29
 Epoch: 50, lr: 3.0e-04, train_loss: 0.0500 | 0.6236 | 3.1681, train_acc: 0.9843 test_loss: 0.0459 | 0.5518 | 2.8049, test_acc: 0.9855, best: 0.9869, time: 0:00:29
 Epoch: 51, lr: 3.0e-04, train_loss: 0.0484 | 0.6220 | 3.1586, train_acc: 0.9843 test_loss: 0.0441 | 0.5453 | 2.7705, test_acc: 0.9860, best: 0.9869, time: 0:00:30
 Epoch: 52, lr: 3.0e-04, train_loss: 0.0494 | 0.6202 | 3.1505, train_acc: 0.9840 test_loss: 0.0452 | 0.5425 | 2.7578, test_acc: 0.9853, best: 0.9869, time: 0:00:29
 Epoch: 53, lr: 3.0e-04, train_loss: 0.0486 | 0.6191 | 3.1443, train_acc: 0.9838 test_loss: 0.0498 | 0.5448 | 2.7739, test_acc: 0.9848, best: 0.9869, time: 0:00:29
 Epoch: 54, lr: 3.0e-04, train_loss: 0.0492 | 0.6174 | 3.1361, train_acc: 0.9845 test_loss: 0.0450 | 0.5412 | 2.7510, test_acc: 0.9851, best: 0.9869, time: 0:00:29
 Epoch: 55, lr: 3.0e-04, train_loss: 0.0491 | 0.6157 | 3.1278, train_acc: 0.9846 test_loss: 0.0457 | 0.5405 | 2.7482, test_acc: 0.9862, best: 0.9869, time: 0:00:29
 Epoch: 56, lr: 3.0e-04, train_loss: 0.0469 | 0.6141 | 3.1173, train_acc: 0.9851 test_loss: 0.0415 | 0.5437 | 2.7601, test_acc: 0.9869, best: 0.9869, time: 0:00:29
 Epoch: 57, lr: 3.0e-04, train_loss: 0.0484 | 0.6133 | 3.1147, train_acc: 0.9846 test_loss: 0.0423 | 0.5380 | 2.7321, test_acc: 0.9855, best: 0.9869, time: 0:00:29
 Epoch: 58, lr: 3.0e-04, train_loss: 0.0486 | 0.6117 | 3.1069, train_acc: 0.9849 test_loss: 0.0397 | 0.5427 | 2.7533, test_acc: 0.9866, best: 0.9869, time: 0:00:29
 Epoch: 59, lr: 3.0e-04, train_loss: 0.0474 | 0.6104 | 3.0993, train_acc: 0.9853 test_loss: 0.0438 | 0.5368 | 2.7276, test_acc: 0.9853, best: 0.9869, time: 0:00:29
 Epoch: 60, lr: 3.0e-05, train_loss: 0.0312 | 0.6071 | 3.0669, train_acc: 0.9900 test_loss: 0.0324 | 0.5382 | 2.7234, test_acc: 0.9889, best: 0.9889, time: 0:00:36
 Epoch: 61, lr: 3.0e-05, train_loss: 0.0246 | 0.6067 | 3.0583, train_acc: 0.9931 test_loss: 0.0311 | 0.5389 | 2.7256, test_acc: 0.9890, best: 0.9890, time: 0:00:36
 Epoch: 62, lr: 3.0e-05, train_loss: 0.0233 | 0.6064 | 3.0551, train_acc: 0.9933 test_loss: 0.0308 | 0.5369 | 2.7155, test_acc: 0.9899, best: 0.9899, time: 0:00:36
 Epoch: 63, lr: 3.0e-05, train_loss: 0.0214 | 0.6061 | 3.0521, train_acc: 0.9939 test_loss: 0.0303 | 0.5379 | 2.7200, test_acc: 0.9902, best: 0.9902, time: 0:00:37
 Epoch: 64, lr: 3.0e-05, train_loss: 0.0209 | 0.6057 | 3.0494, train_acc: 0.9945 test_loss: 0.0302 | 0.5373 | 2.7170, test_acc: 0.9900, best: 0.9902, time: 0:00:29
 Epoch: 65, lr: 3.0e-05, train_loss: 0.0204 | 0.6058 | 3.0492, train_acc: 0.9944 test_loss: 0.0304 | 0.5375 | 2.7181, test_acc: 0.9898, best: 0.9902, time: 0:00:29
 Epoch: 66, lr: 3.0e-05, train_loss: 0.0197 | 0.6057 | 3.0483, train_acc: 0.9949 test_loss: 0.0304 | 0.5366 | 2.7131, test_acc: 0.9901, best: 0.9902, time: 0:00:29
 Epoch: 67, lr: 3.0e-05, train_loss: 0.0198 | 0.6051 | 3.0453, train_acc: 0.9947 test_loss: 0.0306 | 0.5371 | 2.7158, test_acc: 0.9904, best: 0.9904, time: 0:00:36
 Epoch: 68, lr: 3.0e-05, train_loss: 0.0191 | 0.6052 | 3.0451, train_acc: 0.9948 test_loss: 0.0305 | 0.5379 | 2.7199, test_acc: 0.9899, best: 0.9904, time: 0:00:29
 Epoch: 69, lr: 3.0e-05, train_loss: 0.0193 | 0.6052 | 3.0455, train_acc: 0.9949 test_loss: 0.0307 | 0.5379 | 2.7202, test_acc: 0.9898, best: 0.9904, time: 0:00:29
 Epoch: 70, lr: 3.0e-05, train_loss: 0.0184 | 0.6051 | 3.0441, train_acc: 0.9953 test_loss: 0.0309 | 0.5377 | 2.7194, test_acc: 0.9902, best: 0.9904, time: 0:00:29
 Epoch: 71, lr: 3.0e-05, train_loss: 0.0195 | 0.6049 | 3.0442, train_acc: 0.9951 test_loss: 0.0305 | 0.5372 | 2.7168, test_acc: 0.9903, best: 0.9904, time: 0:00:29
 Epoch: 72, lr: 3.0e-05, train_loss: 0.0186 | 0.6047 | 3.0424, train_acc: 0.9953 test_loss: 0.0303 | 0.5371 | 2.7161, test_acc: 0.9899, best: 0.9904, time: 0:00:29
 Epoch: 73, lr: 3.0e-05, train_loss: 0.0195 | 0.6046 | 3.0424, train_acc: 0.9954 test_loss: 0.0302 | 0.5361 | 2.7107, test_acc: 0.9896, best: 0.9904, time: 0:00:29
 Epoch: 74, lr: 3.0e-05, train_loss: 0.0189 | 0.6045 | 3.0414, train_acc: 0.9952 test_loss: 0.0301 | 0.5361 | 2.7107, test_acc: 0.9907, best: 0.9907, time: 0:00:36
 Epoch: 75, lr: 3.0e-05, train_loss: 0.0191 | 0.6041 | 3.0396, train_acc: 0.9954 test_loss: 0.0305 | 0.5380 | 2.7206, test_acc: 0.9902, best: 0.9907, time: 0:00:29
 Epoch: 76, lr: 3.0e-05, train_loss: 0.0183 | 0.6042 | 3.0395, train_acc: 0.9956 test_loss: 0.0312 | 0.5373 | 2.7179, test_acc: 0.9893, best: 0.9907, time: 0:00:29
 Epoch: 77, lr: 3.0e-05, train_loss: 0.0187 | 0.6041 | 3.0392, train_acc: 0.9957 test_loss: 0.0309 | 0.5369 | 2.7153, test_acc: 0.9895, best: 0.9907, time: 0:00:29
 Epoch: 78, lr: 3.0e-05, train_loss: 0.0192 | 0.6039 | 3.0384, train_acc: 0.9954 test_loss: 0.0309 | 0.5375 | 2.7185, test_acc: 0.9898, best: 0.9907, time: 0:00:29
 Epoch: 79, lr: 3.0e-05, train_loss: 0.0189 | 0.6038 | 3.0379, train_acc: 0.9956 test_loss: 0.0308 | 0.5370 | 2.7160, test_acc: 0.9902, best: 0.9907, time: 0:00:29
 Epoch: 80, lr: 3.0e-06, train_loss: 0.0176 | 0.6035 | 3.0350, train_acc: 0.9961 test_loss: 0.0306 | 0.5361 | 2.7109, test_acc: 0.9901, best: 0.9907, time: 0:00:29
 Epoch: 81, lr: 3.0e-06, train_loss: 0.0169 | 0.6036 | 3.0347, train_acc: 0.9964 test_loss: 0.0306 | 0.5366 | 2.7138, test_acc: 0.9897, best: 0.9907, time: 0:00:29
 Epoch: 82, lr: 3.0e-06, train_loss: 0.0171 | 0.6035 | 3.0345, train_acc: 0.9960 test_loss: 0.0304 | 0.5363 | 2.7118, test_acc: 0.9901, best: 0.9907, time: 0:00:29
 Epoch: 83, lr: 3.0e-06, train_loss: 0.0168 | 0.6037 | 3.0352, train_acc: 0.9964 test_loss: 0.0306 | 0.5360 | 2.7107, test_acc: 0.9899, best: 0.9907, time: 0:00:29
 Epoch: 84, lr: 3.0e-06, train_loss: 0.0171 | 0.6034 | 3.0342, train_acc: 0.9965 test_loss: 0.0305 | 0.5360 | 2.7103, test_acc: 0.9901, best: 0.9907, time: 0:00:29
 Epoch: 85, lr: 3.0e-06, train_loss: 0.0173 | 0.6032 | 3.0334, train_acc: 0.9959 test_loss: 0.0305 | 0.5357 | 2.7089, test_acc: 0.9899, best: 0.9907, time: 0:00:29
 Epoch: 86, lr: 3.0e-06, train_loss: 0.0168 | 0.6035 | 3.0343, train_acc: 0.9962 test_loss: 0.0305 | 0.5365 | 2.7129, test_acc: 0.9898, best: 0.9907, time: 0:00:29
 Epoch: 87, lr: 3.0e-06, train_loss: 0.0170 | 0.6034 | 3.0338, train_acc: 0.9964 test_loss: 0.0305 | 0.5364 | 2.7127, test_acc: 0.9894, best: 0.9907, time: 0:00:29
 Epoch: 88, lr: 3.0e-06, train_loss: 0.0168 | 0.6032 | 3.0330, train_acc: 0.9963 test_loss: 0.0306 | 0.5362 | 2.7118, test_acc: 0.9897, best: 0.9907, time: 0:00:29
 Epoch: 89, lr: 3.0e-06, train_loss: 0.0166 | 0.6032 | 3.0329, train_acc: 0.9965 test_loss: 0.0305 | 0.5363 | 2.7120, test_acc: 0.9896, best: 0.9907, time: 0:00:29
 Epoch: 90, lr: 3.0e-07, train_loss: 0.0167 | 0.6033 | 3.0330, train_acc: 0.9964 test_loss: 0.0305 | 0.5362 | 2.7114, test_acc: 0.9896, best: 0.9907, time: 0:00:29
 Epoch: 91, lr: 3.0e-07, train_loss: 0.0171 | 0.6033 | 3.0335, train_acc: 0.9960 test_loss: 0.0305 | 0.5362 | 2.7112, test_acc: 0.9896, best: 0.9907, time: 0:00:29
 Epoch: 92, lr: 3.0e-07, train_loss: 0.0166 | 0.6035 | 3.0342, train_acc: 0.9965 test_loss: 0.0304 | 0.5362 | 2.7113, test_acc: 0.9898, best: 0.9907, time: 0:00:29
 Epoch: 93, lr: 3.0e-07, train_loss: 0.0170 | 0.6034 | 3.0341, train_acc: 0.9963 test_loss: 0.0304 | 0.5363 | 2.7117, test_acc: 0.9897, best: 0.9907, time: 0:00:29
 Epoch: 94, lr: 3.0e-07, train_loss: 0.0165 | 0.6034 | 3.0336, train_acc: 0.9965 test_loss: 0.0304 | 0.5361 | 2.7108, test_acc: 0.9897, best: 0.9907, time: 0:00:29
 Epoch: 95, lr: 3.0e-07, train_loss: 0.0168 | 0.6033 | 3.0335, train_acc: 0.9963 test_loss: 0.0304 | 0.5362 | 2.7113, test_acc: 0.9897, best: 0.9907, time: 0:00:29
 Epoch: 96, lr: 3.0e-07, train_loss: 0.0169 | 0.6034 | 3.0338, train_acc: 0.9960 test_loss: 0.0304 | 0.5362 | 2.7112, test_acc: 0.9896, best: 0.9907, time: 0:00:29
 Epoch: 97, lr: 3.0e-07, train_loss: 0.0164 | 0.6034 | 3.0334, train_acc: 0.9966 test_loss: 0.0304 | 0.5362 | 2.7116, test_acc: 0.9897, best: 0.9907, time: 0:00:29
 Epoch: 98, lr: 3.0e-07, train_loss: 0.0169 | 0.6035 | 3.0344, train_acc: 0.9963 test_loss: 0.0304 | 0.5362 | 2.7116, test_acc: 0.9897, best: 0.9907, time: 0:00:29
 Epoch: 99, lr: 3.0e-07, train_loss: 0.0163 | 0.6033 | 3.0326, train_acc: 0.9967 test_loss: 0.0304 | 0.5361 | 2.7109, test_acc: 0.9897, best: 0.9907, time: 0:00:29
 Highest accuracy: 0.9907